% Interesting trick to instead make the chapter number the letter C for this appendix.
\begingroup
\renewcommand\thechapter{C}
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries}{}{20pt}{\Huge}
\setcounter{section}{0} % Set the section counter back to 0 so that Appendix B doesn't interfere.

\chapter*{Appendix D - Descriptions of metrics}
\addcontentsline{toc}{chapter}{Appendix D - Descriptions of metrics}
\markboth{Appendix D}{}

\section{Accuracy}
Accuracy is the most straightforward metric, measuring the overall correctness of the model's predictions. It is calculated as the ratio of correct predictions
(both true positives [TP] and true negatives [TN]) to the total number of predictions, including false positives [FP] and negatives [FN] \autocite{google_classification_nodate},
shown in Equation \ref{eq:Accuracy}.

\begin{equation}\label{eq:Accuracy}
    \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\end{equation}


\para While accuracy is intuitive and easy to interpret, it can be misleading in cases
of imbalanced datasets, which is a concern in this dataset that will need to be addressed in preprocessing.

\section{Recall}
Recall, also known as sensitivity or true positive rate, is particularly important in medical diagnosis scenarios like diabetes detection. It measures
the model's ability to correctly identify all positive cases. In this context, high recall ensures that we minimize the number of diabetic patients
who are incorrectly classified as non-diabetic (false negatives). This is crucial because missing a diabetes diagnosis could have serious health implications for the patient such 
as neuropathy, vision problems and other health complications such as heart disease or strokes \autocite{nhs_type_nodate}.

\begin{equation}\label{eq:Recall}
    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}

\section{F1 Score}
The F1 score provides a balanced measure of the model's performance by combining precision\footnote{The rate of correctly classified positives. Often has an inverse relationship with Recall \autocite{google_classification_nodate}.}
and recall into a single metric, and is particularly useful with imbalanced datasets such as the one used in this project.
The F1 score is calculated as the harmonic mean\footnote{The reciprocal of the mean of the reciprocals of the features. A reciprocal is 1 divided by the number, such as 4's reciprocal being 1/4.}
of precision and recall, giving equal weight to both metrics, which is useful because in a medical context, false positives and negatives are extremely important 
and can bear significant consequences.

\begin{equation}\label{eq:Precision}
    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\end{equation}

\begin{equation}\label{eq:F1Score}
    \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

