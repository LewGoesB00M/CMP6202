% -------------------------------------------------------------------------------
% Establish page structure & font.
\documentclass[12pt]{report}

\usepackage[total={6.5in, 9in},
	left=1in,
	right=1in,
	top=1in,
	bottom=1in,]{geometry} % Page structure

\usepackage{graphicx} % Required for inserting images
\graphicspath{{images/}} % Any additional images I use (BCU logo, etc) are from here.

\usepackage[utf8]{inputenc} % UTF-8 encoding
\usepackage[T1]{fontenc} % T1 font
\usepackage{float}  % Allows for floats to be positioned using [H], which correctly
                    % positions them relative to their location within my LaTeX code.
\usepackage{subcaption}

% -------------------------------------------------------------------------------
% Declare biblatex with custom Harvard BCU styling for referencing.
\usepackage[
    useprefix=true,
    maxcitenames=3,
    maxbibnames=99,
    style=authoryear,
    dashed=false, 
    natbib=true,
    url=false,
    backend=biber
]{biblatex}

% Additional styling options to ensure Harvard referencing format.
\renewbibmacro*{volume+number+eid}{
    \printfield{volume}
    \setunit*{\addnbspace}
    \printfield{number}
    \setunit{\addcomma\space}
    \printfield{eid}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}

% Declare it as the bibliography source, to be called later via \printbibliography
\addbibresource{report.bib}

% -------------------------------------------------------------------------------
% To prevent "Chapter N" display for each chapter
\usepackage[compact]{titlesec}
\usepackage{wasysym}
\usepackage{import}

\titlespacing*{\chapter}{0pt}{-2cm}{0.5cm}
\titleformat{\chapter}[display]
{\normalfont\bfseries}{}{0pt}{\Huge}

% -------------------------------------------------------------------------------
% Custom macro to make an un-numbered footnote.

\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

% -------------------------------------------------------------------------------
% Fancy headers; used to show my name, BCU logo and current chapter for the page.
\usepackage{fancyhdr}
\usepackage{calc}
\pagestyle{fancy}

\setlength\headheight{37pt} % Set custom header height to fit the image.

\renewcommand{\chaptermark}[1]{%
    \markboth{#1}{}} % Include chapter name.


% Lewis Higgins - ID 22133848           [BCU LOGO]                [CHAPTER NAME]
\lhead{Lewis Higgins - ID 22133848~~~~~~~~~~~~~~~\includegraphics[width=1.75cm]{BCU}}
\fancyhead[R]{\leftmark}

% -------------------------------------------------------------------------------

\title{CMP6202 Report}
\author{Lewis Higgins - Student ID 22133848}
\date{December 2024}

% -------------------------------------------------------------------------------

\begin{document}


\makeatletter
\begin{titlepage}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{BCU}\\[4ex]
        {\large \bfseries  \@title }\\[2ex]
        {\large \bfseries  DRAFT VERSION }\\[2ex]
        {\@author}\\[30ex]
        {Word count: XXXX}\\[20ex]
    \end{center}
\end{titlepage}
\makeatother
\thispagestyle{empty}
\newpage


% Page counter trick so that the contents page doesn't increment it.
\setcounter{page}{0}

\tableofcontents
\thispagestyle{empty}

% Declaring un-numbered chapter because I prefer how it looks.
\chapter*{Introduction}
% Add it to the contents, because un-numbered chapters aren't by default.
\addcontentsline{toc}{chapter}{Introduction}
% Put the chapter name in the header.
\markboth{Introduction}{}

Your EDA can be very extensive, and you could potentially have pages and pages and pages of it; this isn't a bad thing.
The vast majority of any ML-related work is EDA because it gives you the background information on the dataset to then apply
when training the model, such as the identification of non-numerical columns and encoding them into numerical equivalents where
possible so that they become useful training data for the model, as ML models cannot interpret strings.
\\
\noindent
You should work with at least two ML algorithms. Compare them, pros/cons, why you settled on the one you did.
F1-Score if you do a classification problem, otherwise $r^2$. A template exists for both the presentation and report.
\noindent \textbf{You need to pay close attention to section 6.2, as he says it'll be at least 10\%}.
He mentions things like certificates, a linkedin profile and more can be relevant to this section.
Sounds less like "individual learning reflection" and more "individual reflection".
\\
\noindent 
You might be missing some \LaTeX packages, consult your lit review for the ones you might need.
This also applies to the CMP6230 report.
\\
\noindent
Kaggle can give you certificates, as can GitHub. With Github, it could be 
advisable to make this repo public, or perhaps a sanitised duplicate. 

\chapter{Notes, remove before final}
\section{Week 7}
\subsection{Naive Bayes}
This symbol | means "given that".
Sentiment analysis, this could be linked to your dissertation too.
Used in classification. After the formula, the larger number is chosen as the class (i.e. if spam scored 0.05 but good showed 0.12, good wins).
Gaussian NB is best used for continuous data and text data classification.
The prior probability is calculated by dividing the number of positive instances by the total number of instances.
When classifying spam emails, the probability of the email being spam is the prior probability.
Probability of a given event (A) given that (|) a previous event has occurred (B)

\subsubsection{Cross Validation}
Train test split has some issues, due to its random selection of data to put in each split, which are addressed by cross validation.
Cross validation helps to reduce bias in an ML model.
\subsubsection{K-Fold}
Divides the dataset into "K" subsets/folds, then trains and evaluates the model "K" times, using a different fold as the test set while the others 
act as the test data.
\subsubsection{Stratified K-Fold}
A variation of K-Fold that ensures class balance, helping where the dataset is imbalanced. \textbf{Could be very relevant.}
Week 7 PPT slides 17 through 20 help with this and regular K-Fold.
\subsubsection{Accuracy score}
Your accuracy score will differ from the cross-validation score because it doesn't account for the averaged and varied subsets
used in cross-validation.

\subsection{Decision tree}
Another classification method, currently don't know much about it.
Initialised in code via DecisionTreeClassifier().

\section{Week 8}
You left the lecture early this week to work on your dissertation. He may have covered topics that you missed by
consequence. I believe all of these are classification algorithms, and as such are of key importance to your probable 
dataset.

\subsection{Random Forest}
Utilise multiple deicsion trees. Random data points are selected and a decision tree is built with them. Several
trees are then modeled to predict the value of any new data point. Because there's multiple trees, there's multiple 
outputs, so you have to average all the predicted values for a new data point to compute the final output.

\subsection{Logistic Regression}
I really don't understand this in the slightest.
Takes iterations as a parameter to "find the best line"?

\subsection{Support Vector Machines}
Also didn't understand this. Powerpoints are useless.


\subsection{Dataset imbalance}
\subsubsection{Oversampling}
SMOTE adds synthetic data to add more to the minority class. It's based on nearest-neighbours.
Experimentation in labs revealed that there seems to be an element of RNG to it that could then 
influence the accuracy of models. In your work, perhaps specify a seed so that your results 
can be reproduced.

\subsubsection{Undersampling}
Undersampling removes some of the majority. Good with loads of data (i.e. millions).
Uses TomekLinks, though the Powerpoint does not say what that is.

\section{Weeks 10, 11 and 12?}
No notes for week 9 because I was doing CMP6230 instead. Consult lab sheet for it because that's equally useful.

It sounds like going forward you're not learning things relevant to the module anymore? Just him loredumping about various 
tech like deep learning.

\subsubsection{Epochs}
A common term that you see quite often. Refers to iterations. Typically you'll see it in relation to neural networks 
where each epoch was one attempt that then influences the next.

\section{Mihai W7}
\subsection{Overall notes - Beginning of talk}
\begin{itemize}
    \item Find a problem before a dataset (No)
    \item Identify if said problem is regression or classification
    \item It's hard to use a classification dataset where the target column is just a bunch of labels (???)
    \begin{itemize}
        \item This may be in reference to Week 7's wine set where you had 7 different levels of quality but converted them to 2 for binary classification.
    \end{itemize}
    \item Examples on Moodle
    \item Datasets on Moodle but it's almost 100\% that someone else will have used them.
    \item I also don't know if you're even allowed to use them.
    \item A dataset can have more than one target column (though the ones you use likely won't).
    \item Mihai strongly warns that ML is a very iterative process, and your first attempt will probably be poor. \begin{itemize}
        \item This is why you use \textbf{pipelines} as in CMP6230 but that's not this module.
    \end{itemize}
    \item You will be asked questions on it in the presentation \textbf{RELATED TO WHAT YOU'VE DONE IN CLASS}, likely
    by Mihai himself, and he is trying to catch you out.
\end{itemize}

\subsection{Section 2}
\begin{itemize}
    \item After you have an identified dataset, begin EDA.
    \item Describe how you split the dataset into training and testing dataset.
    \item If the machine sees the test data, it may "overfit". \begin{itemize}
        \item You've seen this before where the line of best fit isn't really a line and just connects every single point, meaning it's 
        really good at guessing the data it already knows but not new data. 
    \end{itemize}
    \item You split your data \textit{BEFORE} EDA to avoid "conceptual overfitting".
    \item When splitting data, you need to think about data imbalance i.e. training set having too much of the one option and few of the other (too many True, not enough False).
    \item SKLearn may try to fix that for you.
    \item Identify outliers, missing data.
    \item "Missing information can be information in itself". Consider the source of the data (not Kaggle but rather where the Kaggle author got it from)
    \begin{itemize}
        \item You might be able to impute data rather than deleting it.
        \item Conserve as much data as you can, deleting data should be a last option.
    \end{itemize}
    \item Erroneous data \begin{itemize}
        \item Another reason why your data source is important.
        \item Could just be mistyped, see what the erroneous data actually is to see if you can correct it.
    \end{itemize}
    \item Outliers
    \begin{itemize}
        \item Is it significant enough to remove it? Is it definitely an outlier; could it feasibly be true? (speed camera example where one guy went 30 but another went 100, but that's still plausible.)
        \item Boxplots can identify outliers.
    \end{itemize}
    \item If your dataset is bad, your model will be, too.
\end{itemize}

\subsection{Section 3}
\begin{itemize}
    \item Identify the right algorithm. \begin{itemize}
        \item Decision trees are good at classification.
        \item Random forest is also used for it.
        \item Naive Bayes and KNN work for prediction and classification.
        \item Some of these may perform worse with higher amounts of data, look into them.
    \end{itemize}
    \item Performance won't matter a massive amount but you still need to be able to justify why it was good.
    \begin{itemize}
        \item Also justify its downsides and limitations.
        \item And the limtiations of the dataset itself.
    \end{itemize}
\end{itemize}

\subsection{Section 4}
\begin{itemize}
    \item Encoding is important here because ML doesn't use text. \begin{itemize}
        \item Side-note: Even if it does that's actually just an abstraction and it's just encoding it under the hood.
    \end{itemize}
    \item Fine-tuning \begin{itemize}
        \item Playing around with parameters of the functions.
        \item KNN Neighbours and such
        \item Often comes after an initial test run
        \item If your accuracy is really low (20\% was the example), fine-tuning won't help and you just need to redo the entire work.
        \item Could maybe give you an extra 5\% accuracy.
        \item Decision trees may have multiple versions. SKLearn's decision tree may be different from a CUDA one.
    \end{itemize}

    \item Evaluation metrics
    \begin{itemize}
        \item Accuracy is not Precision. Which do you need?
        \item You want both to be high, if one lags massively behind the other then that's bad.
        \item ML is an iterative process until you can get the best performing model.
    \end{itemize}
\end{itemize}

\subsection{Section 5}
    \begin{itemize}
        \item Visualisation of evaluation results
        \begin{itemize}
            \item For a correlation matrix, a heatmap is better than a bar plot for example.
        \end{itemize}
    \end{itemize}

\subsection{Presentation}
When writing this presentation, there's 3 questions at all times. What, why and how?
What are you doing, why are you doing it, how are you doing it?
Why is very important - why did I select my problem? Why do I think it's important.
Panel may consist of many people. Nouh and his students

\chapter{Key note}
You need quite a substantial amount of this actually done for \textbf{November 29th} rather than Dec 20th as the 
deadline states. This is because the presentation relies upon this report. For both the report and the presentation,
treat it like your dissertation. Cite some academic papers to prove you're making a good case. Mihai used the 
term "literature review", so maybe he really does mean to that depth.

Furthermore, while it may not actually be marked, describe everything you do within the Jupyter notebook with 
Markdown text cells. It won't count to the word count and might let you sneak in some added detail that might not 
have been able to go in the report.

\end{document}