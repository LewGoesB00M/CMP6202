% -------------------------------------------------------------------------------
% Establish page structure & font.
\documentclass[12pt]{report}

\usepackage[total={6.5in, 9in},
	left=1in,
	right=1in,
	top=1in,
	bottom=1in,]{geometry} % Page structure

\usepackage{graphicx} % Required for inserting images
\graphicspath{{images/}} % Any additional images I use (BCU logo, etc) are from here.

\usepackage[utf8]{inputenc} % UTF-8 encoding
\usepackage[T1]{fontenc} % T1 font
\usepackage{float}  % Allows for floats to be positioned using [H], which correctly
                    % positions them relative to their location within my LaTeX code.
\usepackage{subcaption}
\usepackage{amsmath}

% -------------------------------------------------------------------------------
% Declare biblatex with custom Harvard BCU styling for referencing.
\usepackage[
    useprefix=true,
    maxcitenames=2,
    maxbibnames=99,
    style=authoryear,
    dashed=false, 
    natbib=true,
    url=false,
    backend=biber
]{biblatex}

% Additional styling options to ensure Harvard referencing format.
\renewbibmacro*{volume+number+eid}{
    \printfield{volume}
    \setunit*{\addnbspace}
    \printfield{number}
    \setunit{\addcomma\space}
    \printfield{eid}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}

% Declare it as the bibliography source, to be called later via \printbibliography
\addbibresource{Report.bib}

% -------------------------------------------------------------------------------
% To prevent "Chapter N" display for each chapter
\usepackage[compact]{titlesec}
\usepackage{wasysym}
\usepackage{import}

\titlespacing*{\chapter}{0pt}{-2cm}{0.5cm}
\titleformat{\chapter}[display]
{\normalfont\bfseries}{}{0pt}{\Huge}

% -------------------------------------------------------------------------------
% Custom macro to make an un-numbered footnote.

\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

% -------------------------------------------------------------------------------
% Fancy headers; used to show my name, BCU logo and current chapter for the page.
\usepackage{fancyhdr}
\usepackage{calc}
\pagestyle{fancy}

\setlength\headheight{37pt} % Set custom header height to fit the image.

\renewcommand{\chaptermark}[1]{%
    \markboth{#1}{}} % Include chapter name.


% Lewis Higgins - ID 22133848           [BCU LOGO]                [CHAPTER NAME]
\lhead{Lewis Higgins - ID 22133848~~~~~~~~~~~~~~~\includegraphics[width=1.75cm]{BCU}}
\fancyhead[R]{\leftmark}

% ------------------------------------------------------------------------------
% Used to add PDF hyperlinks for figures and the contents page.

\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=black,
}

% ------------------------------------------------------------------------------
\usepackage{xcolor} 
\usepackage{colortbl}
\usepackage{longtable}
\usepackage{amssymb}
% ------------------------------------------------------------------------------
\usepackage{tcolorbox}
\newcommand{\para}{\vspace{8pt}\noindent}
\usepackage{tikz}
% -------------------------------------------------------------------------------

\title{Predicting the presence of diabetes mellitus using supervised learning models}
\author{Lewis Higgins - Student ID 22133848}
\date{December 2024}

% -------------------------------------------------------------------------------

\begin{document}


\makeatletter
\begin{titlepage}
    \begin{center}
        \includegraphics[width=.8\linewidth]{BCU-Long.jpg}\\[4ex]
        {\huge \bfseries CMP6202}\\[4ex]
        {\huge \bfseries Artificial Intelligence \& Machine Learning}\\[4ex]
        {\huge \bfseries 2024–2025}\\[16ex]
        {\huge \bfseries  \@title}\\[35ex]
        {\@author}\\[2ex]
        {Module Coordinator: Nouh Elmitwally}\\[2ex]
        {\bfseries Word count excluding figures, tables and appendices: 4,379}\\[10ex]
    \end{center}
\end{titlepage}
\makeatother
\thispagestyle{empty}
\newpage


% Page counter trick so that the contents page doesn't increment it.
\setcounter{page}{0}


\tableofcontents
\thispagestyle{empty}

\begin{abstract}
    This report presents a comprehensive study on predicting the presence of diabetes mellitus using supervised learning models. 
    The research integrates two datasets, the Pima Indian Diabetes Database and a Frankfurt diabetes dataset, resulting in a combined dataset of
    2,768 samples. Extensive exploratory data analysis revealed key insights into feature relationships and data quality issues, which were 
    addressed through preprocessing techniques including outlier handling, missing data imputation with KNN, and class balancing with SMOTE. 
    Five classification algorithms were developed and evaluated: Random Forest, Support Vector Machine, Logistic Regression, Naïve Bayes, and K-Nearest Neighbours.
    The models had an initial iteration with reasonably good performance, which was further improved in a second iteration of each model using stratified cross-validation
    and hyperparameter tuning. K-Nearest Neighbours emerged as the top-performing model, achieving 97.8\% accuracy, 96.2\% recall, and an F1-score of 96.6 on the unseen test set,
    with this performance closely followed by Random Forest. The study demonstrates the potential of machine learning in the medical field, while also highlighting the 
    importance of proper data preprocessing and model optimization techniques. 
\end{abstract}


\chapter{Introduction}

% You've opened by talking about the UK but then transitioned to the whole world?
% Your datasets are American and German in origin, so it's likely that the worldwide perspective will be better.
Type 2 diabetes, or diabetes mellitus, accounts for 90\% of the 4.4 million cases of diabetes in the UK, with estimations that 
there are 1.2 million undiagnosed cases of the condition across the country \autocite{diabetes_uk_how_nodate}. The condition's 
occurrence per 100,000 individuals is rapidly increasing, with \textcite{khan_epidemiology_2020}'s analysis projecting that by 
2030, the rate will reach 7,079 per 100,000. Many people with diabetes suffer immensely reduced quality of life, with approximately 50\% 
of patients suffering from peripheral neuropathy \autocite{dhanapalaratnam_effect_2024}, an irreversible disability causing immense pain due 
to nerve damage from high blood sugar \autocite{nhs_peripheral_2022}, which can occur when the patient was unaware they even 
had diabetes. 

\para
Therefore, it is imperative that systems are put in place to enable the swift diagnosis of diabetes mellitus. 
This can be accomplished by training machine learning models on existing clinical datasets 
to identify common trends in those with and without the condition. This report will document the planning, development 
and evaluation of five machine learning models to classify diabetes mellitus based on multiple clinical factors, specifically through the stages of:

\begin{itemize}
    \item Dataset Identification
    \item Data Integration
    \item Data Preprocessing
    \item Exploratory Data Analysis (EDA)
    \item Model Development 
    \item Model Evaluation
    \item Research Conclusions
\end{itemize}

\pagebreak 
\section{Dataset Identification}

Machine learning models require lots of training data, meaning a dataset must be identified consisting of many 
rows and features. Two datasets were identified which could be integrated into one larger dataset, the first of which being 
the Pima Indian Diabetes Database \autocite{uci_machine_learning_pima_nodate}, downloaded from \href{https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database}{Kaggle}.
The data originates from the National Institute of Diabetes and Digestive and Kidney Diseases, who collected this data from Pima Indian\footnote{"Pima Indian" refers to a specific Native American ethnic group rather than people from India.}
women aged 21 and over in hospitals in Phoenix, Arizona, USA. It has seen wide use across academic literature relating to machine learning 
where other researchers have also performed diabetes classification via supervised learning \autocite{alzubi_diabetes_2023,zou_construction_2024,joshi_predicting_2021,hayashi_rule_2016}.
This dataset contains 768 rows with 9 features.

\para
The second dataset is also from \href{https://www.kaggle.com/datasets/johndasilva/diabetes/data}{Kaggle}, and has been previously used in literature by 
\textcite{zou_construction_2024}. This dataset \autocite{john_dasilva_frankfurt_nodate} is based on data from female patients in Frankfurt, Germany, and
includes the same 9 features as the Pima Indian dataset, but with 2000 samples. By integrating these two datasets into one larger
dataset of 2768 rows, it will be possible to give the machine learning models more data to train upon.

\para Table \ref{tab:Features} details the 9 features seen in both datasets and their descriptions.

\begin{longtable}{ | p{0.2\textwidth} | p{0.5\textwidth} | p{0.2\textwidth} |}
    \hline
    \cellcolor{blue!25}Feature & \cellcolor{blue!25}Description & \cellcolor{blue!25}Measurement \\
    \hline
    Pregnancies & The number of pregnancies the patient has had. & Ratio\\
    \hline
    Glucose & Plasma glucose concentration over 2 hours in an oral glucose tolerance test. & Ratio \\
    \hline
    BloodPressure & Diastolic blood pressure in mm/Hg. & Ratio \\
    \hline
    SkinThickness & Triceps skin fold thickness (mm) & Ratio \\
    \hline
    Insulin & 2-hour serum insulin. & Ratio \\
    \hline
    BMI & Body Mass Index, calculated from the patient's weight and height. & Ratio \\
    \hline
    DiabetesPedigree-\newline Function & The product of a function to ascertain the probability of diabetes based on family genetics. \autocite{akmese_diagnosing_2022}
    & Ratio \\
    \hline
    Age & The patient's age. & Ratio\\
    \hline
    Outcome & Whether the patient is likely to develop diabetes. & Nominal\\
    \hline 
    \caption{The features seen in both datasets.}\label{tab:Features}
\end{longtable}

\pagebreak

\section{Supervised learning task identification}
Because patients can have diabetes without knowing,
it is paramount that swift and simple diagnosis methods are put in place, which can be achieved 
through the use of binary classification models. This requires the existence of the 
"ground truth": the label given to data that indicates its class \autocite{c3ai_what_nodate}. 
Within these datasets, the ground truth is present as the 'Outcome' feature, which was used 
as the target variable for the models.

\chapter{Exploratory Data Analysis}
\section{Data Integration}
% Not included in template but I feel it's necessary to include somewhere because that's what you're doing.
The two datasets must first be merged into one to allow for an overall analysis to be performed.
This was a simple process because they both contain the same 9 features.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Integration.png}
    \caption{Integrating the two separate datasets into one larger dataset.}
    \label{fig:Integration}
\end{figure}

\pagebreak 

\section{Question identification and assumptions}
% What questions do you have that thorough EDA would answer?
% Example A gives a good example on what this section should look like.
The key factors involved in the diagnosis of diabetes are critical to understand, which 
can be solved through EDA on these datasets. It is possible to make various assumptions based 
on topical background research of each of the features in the dataset, detailed in Table \ref{tab:Assumptions}

\begin{longtable}{ | p{0.19\textwidth} | p{0.75\textwidth} | }
    \hline
    \cellcolor{blue!25} Feature & \cellcolor{blue!25} Research-based assumptions \\
    \hline
    Pregnancies & Approximately 13.4\% of pregnant women develop a temporary condition known as Gestational 
    Diabetes Mellitus (GDM), which typically subsides after birth \autocite{adam_pregnancy_2023}.
    However, research by \textcite{dennison_absolute_2021} indicates that 33\% of women who develop 
    GDM will go on to develop permanent diabetes mellitus within 15 years. Therefore, it is assumed 
    that pregnancies will positively correlate with the diabetes outcome. It is also expected that 
    pregnancies should naturally positively correlate with age. \\
    \hline
    Glucose & Glucose concentrations are an enormous factor in the diagnosis of diabetes mellitus, being one 
    of the main metrics used to certify the condition, where results over 200mg/dL mean an absolute diagnosis\footnote{The other main metric is insulin deficiency, meaning that the patient could have glucose levels lower than 200mg/dL and still be diagnosed if they are instead insulin deficient. \autocite{aftab_cloud-based_2021}.}
    \autocite{aftab_cloud-based_2021}. It is therefore assumed that the glucose concentrations will be one 
    of the strongest influences of the outcome, and that it will also correlate heavily with insulin levels. \\
    \hline
    BloodPressure & Diastolic blood pressure (DBP) does influence the diagnosis of diabetes mellitus, as 
    56.2\% of recently diagnosed patients presented with elevated DBP in \textcite{nelaj_high_2023}'s limited 
    study of 126 patients, but it is not a decisive factor by itself. Therefore, it is assumed that there will 
    be some correlation between DBP and the outcome, but not as major as other factors like plasma glucose levels.\\
    \hline
    SkinThickness & It is a frequent assumption even non-academically that people who weigh more, and by consequence have higher 
    skin thickness in certain areas such as the triceps, have a higher risk of developing conditions like type 2 diabetes. This is 
    backed by a study by \textcite{ruiz-alejos_skinfold_2020}, which found strong associations between skin thickness and diabetes mellitus,
    as well as high blood pressure. Therefore, it is assumed that there will be a strong correlation between tricep skin thickness and the outcome, 
    as well as an expectation of strong correlations between thickness, BMI and blood pressure.\\
    \hline
    Insulin & Diabetes mellitus is directly associated with insulin deficiency, and as such, it is assumed that this factor will be 
    the strongest influence in the outcome. This is because 2-hour serum insulin tests, as used in this dataset, are frequently 
    part of HOMA-IR\footnote{Homeostasis Model Assessment of Insulin Resistance, used to measure insulin resistance \autocite{tahapary_challenges_2022}, which can be used in both type 1 and type 2 diabetes diagnosis \autocite{khalili_are_2023}.}
    assessments. \\
    \hline
    BMI & BMI is likely to be a significant factor in the outcome, which is backed by previous academic studies indicating that 71\% 
    of studied individuals showed increases in BMI prior to diagnosis \autocite{donnelly_trajectories_2024}. Additionally, BMI is used in insulin resistance measurement assessments,
    which are key assessments in diabetes diagnosis, meaning that it is a safe assumption that BMI will be a large factor in the outcome. \\
    \hline
    DiabetesPedigree- Function & People are more likely to develop diabetes mellitus if there is a family genetic history of the condition, though it is not 
    directly caused by any one particular gene \autocite{diabetes_uk_what_nodate}. With the pedigree function aiming 
    to quantify the inheritance probability, it can be assumed that it will likely correlate heavily with the outcome. \\
    \hline
    Age & \textcite{chackrewarthy_age_2012} studied the effects of age as a risk factor for diabetes mellitus, finding that many natural associated 
    factors of ageing including increases in body fat and decreases in lipid metabolism had considerable influence on the development of insulin 
    resistance and diabetes mellitus by consequence. Therefore, it is likely that there will be a noticeable correlation between a patient's age 
    and the outcome. 
     \\
    \hline
    \caption{Research-based assumptions prior to any EDA.}\label{tab:Assumptions}
\end{longtable}

\para Based on these assumptions, the questions that this EDA process aims to answer are:


\begin{longtable}{ | p{0.05\textwidth} | p{0.85\textwidth} | }
    \hline
    \cellcolor{blue!25} ID & \cellcolor{blue!25} Research-based assumptions \\
    \hline
    1 & Are there any missing values or values that are not physically possible?\\
    \hline
    2 & Are there any significant outliers?\\
    \hline 
    3 & Is the dataset evenly balanced in terms of the outcome? If not, what should be done?\\
    \hline 
    4 & Does the rate of diabetes positively correlate with the amount of pregnancies a woman has had?\\
    \hline
    5 & Does the amount of pregnancies influence any of the other features?\\
    \hline
    6 & What is the distribution of blood glucose levels in patients with and without diabetes?\\
    \hline
    7 & Does BMI influence glucose levels?\\
    \hline
    8 & Is diastolic blood pressure a worthwhile diagnosis method in this dataset?\\
    \hline
    9 & Is the average skin thickness of those with diabetes actually higher than those without?\\
    \hline
    10 & How does the relationship between insulin and glucose change between those with and without diabetes?\\
    \hline
    \caption{The questions that this EDA process aims to answer.}\label{tab:Questions}
\end{longtable}
\pagebreak

\section{Splitting the dataset}
% How was the data split?
% Why do we split? Why 80:20 as you're doing?
% Train/test? KFold?
% Maybe do train/test at first and KFold in an iterative improvement and compare the models?
% Training and testing sets are a minimum. It's possible they also want a VALIDATION set
It is good practice to first split the data into training and testing sets before performing exploratory data analysis
to avoid conceptual overfitting, also known as data leakage. Conceptual overfitting occurs when insights gained from the entire dataset 
influence model development decisions, which may eventually lead to actual overfitting. By excluding the training data from the analysis,
it effectively simulates a real-world environment where the data being given to the model is not known, even to its developers.

% !!! Give a better and longer definition of overfitting.

\para Splitting the data is a mandatory process when developing supervised learning models, primarily for the prevention of overfitting.
Overfitting is a significant challenge in machine learning, where models can perform exceptionally well on their original training data but 
are unable to generalize to unseen data, making them unsuitable for deployed use. The training set must consist of a large portion of the 
data so that the model has enough information to analyse and discover trends within, whereas the testing set is a smaller, unseen remainder 
of the data that the model's predictions can be evaluated against using various metrics. A key point of determination is the proportions 
of the dataset that should go in each set - there is no 'one-size-fits-all' percentage that can provide the best results for every possible dataset \autocite{sivakumar_trade-off_2024},
and factors such as the size of the dataset play a large part in this. Most commonly, splits are either 70:30 or 80:20 for training and testing sets 
respectively.

\para The integrated dataset for this project is 2,768 rows. This is considered to be a small dataset, and as such, it will be best to 
maximize the size of the training split, so a split of 80\% training and 20\% testing was used, visualized in Figure \ref{fig:TrainTestDiagram}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        full/.style={draw, rectangle, minimum width=10cm, minimum height=1.5cm, fill = lightgray},
        train/.style={draw, rectangle, minimum width=8cm, minimum height=1.5cm, fill = cyan},
        test/.style={draw, rectangle, minimum width=2cm, minimum height=1.5cm, fill = pink},
        arrow/.style={<->, thick}
    ]
    
    % Full dataset block
    \node[full] (full) at (0,1.5) {Full dataset (2768 rows)};
    
    % Training set block
    \node[train] (train) at (-1,0) {Training};
    
    % Testing set block
    \node[test] (test) at (4, 0) {Testing};

    % Proportion indicators
    \draw[arrow] (-5,-1.2) -- (2.9,-1.2) node[midway, below] {80\%};
    \draw[arrow] (3.1,-1.2) -- (4.9,-1.2) node[midway, below] {20\%};
    
    \end{tikzpicture}
    \caption{A visual representation of the train/test split.}
    \label{fig:TrainTestDiagram}
\end{figure}

\para To accomplish this, the data must be split into $X$ and $y$ tables, where $X$ consists of the eight features, and $y$ is the target variable.
After the data is split to $X$ and $y$, it can be split into training and testing sets through Scikit-Learn's "train\_test\_split" method, as depicted in Figure \ref{fig:TrainTestSplit}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{EDA/TrainTestSplit.png}
    \caption{Splitting the data at an 80:20 ratio.}
    \label{fig:TrainTestSplit}
\end{figure}

\para By default, this method will shuffle all rows in the dataset before splitting it, which introduces an element of randomness, damaging
reproducibility. To combat this, the "random\_state" parameter was set to ensure that the same shuffle will occur every time.

\para When performing EDA, the Outcome column will be necessary to the analysis, so a deep copy\footnote{A complete copy rather than a pointer to the original data}
of X\_train was made with y\_train (the Outcome column) being added to it, merging the two back into a full training set, shown in Figure \ref{fig:FullTrainingSet}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{EDA/FullTrainingSet.png}
    \caption{Duplicating X\_train and adding the Outcome column for EDA.}
    \label{fig:FullTrainingSet}
\end{figure}


\section{EDA process and results}
The full EDA process and results can be found in Appendix A. They were performed using the Seaborn Python library,
which produces intepretable visualisations to answer each question posed in Table \ref{tab:Questions}. The topics and issues identified
during the process are solved in Section \ref{sec:Preprocessing}.


\section{EDA conclusions}
The extensive EDA conducted in Appendix A provided clear answers to each question posed in Table \ref{tab:Questions}, 
which are detailed below in Table \ref{tab:Answers}.

\begin{longtable}{ | p{0.05\textwidth} | p{0.85\textwidth} | }
    \hline
    \cellcolor{blue!25} ID & \cellcolor{blue!25} EDA-based answer \\
    \hline
    1 & There are missing values in the dataset that were not originally presented as such; they were instead
    0s in columns where this would be physically impossible. After converting the impossible 0s to missing values 
    with NumPy, it was established that there were 18 missing Glucose values, 125 missing BloodPressure values,
    800 missing SkinThickness values, 1330 missing Insulin values, and 39 missing BMI values.
    % \begin{itemize}
    %     \item 18 missing Glucose values.
    %     \item 125 missing BloodPressure values.
    %     \item 800 missing SkinThickness values.
    %     \item 1330 missing Insulin values.
    %     \item 39 missing BMI values.
    % \end{itemize}
    \\
    \hline
    2 & The analysis identified significant outliers across various features using box plots. 
    Particularly extremely outliers were noted in the SkinThickness, BMI, and Insulin columns. These outliers
    could affect data scaling and model performance, and will need to be transformed in data cleaning. \\
    \hline 
    3 & The dataset is imbalanced, with 65.5\% of rows in the training set having an outcome of 0, while the remaining 
    35.5\% had an outcome of 1.\\
    \hline 
    4 & The EDA did show a somewhat positive correlation between the number of pregnancies and diabetes outcomes, 
    although pregnancies alone are not decisive in predicting diabetes.\\
    \hline
    5 & Pregnancies are found to correlate with age, as expected, but do not significantly influence other features.\\
    \hline
    6 & There is a clear and significant distinction in glucose levels between diabetic and non-diabetic individuals,
    with higher levels being mostly present in those with diabetes, making it a strong indicator.\\
    \hline
    7 & A slight positive correlation is observed between BMI and glucose levels, indicating that higher BMI could be 
    associated with higher glucose levels.\\
    \hline
    8 & Blood pressure shows some correlation with diabetes outcomes but is not a strong standalone diagnostic factor. 
    However, like pregnancies, previously mentioned academic research specifies that it is still a useful feature to keep.\\
    \hline
    9 & Individuals with diabetes tend to have higher skin thickness on average, although this feature contains significant high outliers
    in both individuals with and without diabetes.\\
    \hline
    10 &  The relationship between insulin and glucose varies between diabetic and non-diabetic individuals, with diabetic individuals 
    showing higher glucose levels as a result of lower insulin levels, which is expected of type 2 diabetes.\\
    \hline
    \caption{Answers to the questions after the completion of the EDA process.}\label{tab:Answers}
\end{longtable}

\pagebreak
\para Overall, the EDA process conducted in Appendix A has been highly beneficial for many reasons. Firstly, it identified critical data issues, 
such as the abundance of missing values and outliers, which are essential to address to improve model accuracy and reliability. By recognizing these 
issues early, the EDA ensures that data preprocessing steps can be effectively planned to mitigate their impact. Secondly, the EDA highlighted significant
correlations between features and the diabetes outcome, such as the strong influence of glucose levels and BMI on diabetes diagnosis. Understanding these
relationships helps in feature selection and engineering, which are crucial for building effective predictive models. Additionally, the EDA revealed class 
imbalances in the dataset, guiding the development of strategies to handle this imbalance during model training to prevent biased predictions. 

\para The insights gained from the EDA therefore provide a solid foundation for informed decision-making in the experimental design and model development, 
ensuring that the models are trained on clean, balanced, and relevant data, ultimately enhancing their predictive performance. This is especially crucial 
given that the models would be used in the medical field, where incorrect predictions could have life-changing ramifications.

\chapter{Experimental Design} % This may benefit from being its own .TeX file.

\section{Identification of chosen algorithms}\label{sec:ChosenAlgorithms}
% What algorithms? 
% Why those? 
% How do those algorithms work?
Five classification algorithms will be leveraged on this dataset, these being Random Forests,
Support Vector Machines, Logistic Regression, Na\"ive Bayes, and K-Nearest Neighbours. These were 
selected due to their previous use in literature based on these datasets, and also due to their 
high prevalence across many fields. Detailed descriptions of each algorithm, including their 
mathematical formulae, can be found in Appendix B.

\section{Identification of appropriate evaluation techniques}\label{sec:ChosenMetrics}
% What metrics?
% Why those?
% How do they show the model's performance?
When evaluating classification models, it is crucial to select appropriate metrics that provide a comprehensive understanding of the model's performance.
For this project, three key evaluation metrics will be used: accuracy, recall, and F1 score. 
By using these three metrics in combination, we can gain a comprehensive understanding of our classification models' performance. Accuracy provides an overall measure of correctness,
recall ensures positive cases are not missed, and the F1 score offers a balanced view that accounts for both precision and recall.
This multi-metric approach will allow for a detailed evaluation of the five models. Detailed descriptions 
of each metric and how it is calculated can be found in Appendix C.


\section{Data Cleaning and Pre-processing Transformations}\label{sec:Preprocessing}
\subsection{Data encoding}
Data encoding was not necessary in this dataset as all features were already numeric. However, a detailed description of typical 
encoding processes can be found in Appendix D.

\subsection{Data cleaning}
The EDA revealed the necessity for data cleaning in this dataset through the transformation of outliers and imputation of missing values.

\subsubsection{Outlier handling}
It was observed that all columns had outliers, with some having particularly extreme outliers that caused massive variance in the data and 
the visualisations produced from it. To address these, data was constrained to be within the 1st and 99th percentiles, and any data outside 
of these would be increased or decreased to one of these boundaries.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Preprocessing/OutlierConstraining.png}
    \caption{Using np.clip to transform values less than or greater than the 1st and 99th percentiles.}
    \label{fig:OutlierConstraining}
\end{figure}

\subsubsection{Missing data imputation}
After handling the outliers in the data, missing data can be imputed. The selected method for this was KNN imputation,
which uses the KNN algorithm to aggregate data from the $k$ nearest data points and imputes the average of these points 
in place of the missing value \autocite{trainindata_knn_2024}. This method can be very useful as it will not impute many rows 
of the same number as mean or median imputation would do, and instead imputes reasonable values based on similar rows,
which is especially beneficial given the high correlations between some of the missing data (such as Insulin) and data 
that is mostly present (such as Glucose).

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\linewidth]{Preprocessing/KNNImputation.png}
    \caption{Using KNN imputation to resolve missing data.}
    \label{fig:KNNImputation}
\end{figure}

\para K was set to 6 in the imputer to increase the amount of data each imputed point is based on, but also to not average 
too many data points, as the data still varies despite containing many less outliers. 
% Bad description, scrap entirely? 6 was chosen randomly, maybe you could go way higher now with outlier removal.

\subsection{Data scaling}
After imputing missing data, scaling can occur. Algorithms such as KNN and SVMs rely heavily on the distance 
between data points, meaning they can be heavily skewed by large distances between real numbers. Due to the high 
variance in the dataset, it was decided that standardisation will be the scaling procedure. Standardisation uses the 
formula in Equation \ref{eq:Standardisation} to scale data so that it has a mean of 0 and a standard deviation of 1.

\begin{equation}\label{eq:Standardisation}
    Z = \frac{X - \mu}{\sigma}
\end{equation}
\begin{align*}
    Z & \text{ is the standardized value (also known as "z-score")} \\
    X & \text{ is the original value of the feature} \\
    \mu & \text{ is the mean of the feature} \\
    \sigma & \text{ is the standard deviation of the feature}
\end{align*}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Preprocessing/Standardisation.png}
    \caption{Using a StandardScaler to standardise the data.}
    \label{fig:Standardisation}
\end{figure}

\pagebreak

\subsection{Data balancing}
The dataset is imbalanced, containing more samples without diabetes. Therefore,
the Synthetic Minority Oversampling Technique (SMOTE) was used to generate synthetic data to balance the classes. 
SMOTE also uses the KNN algorithm to generate data, but randomly selects nearest neighbours and generates data in-between
the selected sample and the neighbour \autocite{trainindata_overcoming_2023}. This is repeated until the classes are balanced.

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{Preprocessing/SMOTE.png}
    \caption{Using SMOTE to balance the training set.}
    \label{fig:SMOTE}
\end{figure}

\para SMOTE is not used to balance the testing set, as the testing set should be exclusively real data
so that an accurate representation of the model's ability to predict real data can be gathered \autocite{ozbun_properly_2021}.



\section{Limitations and Options}
% What are the limitations of this approach?
%   The sheer amount of missing insulin data and imputation means the datasets are basically synthetic now.
A key limitation in this design comes from the datasets themselves; they contain colossal amounts of missing data, 
with almost half of the Insulin values missing. With Insulin being such an integral part of type 2 diabetes diagnosis,
the issue of having so much missing data cannot be overlooked. While it was addressed using KNN imputation, the synthetic 
data generated by this approach could possibly be of poor quality.

\para Furthermore, SMOTE was used to balance the classes. While SMOTE is a useful tool, it can introduce noise to a dataset.
Additionally, SMOTE was performed after the KNN imputation, meaning that a substantial amount of the dataset was then synthetic data.

\para In future projects, it would be best to use more organised and clean datasets, where such substantial 
preprocessing measures would not be necessary.

\chapter{Model Development} % Titled "predictive modelling / model development" in template.

\section{Predictive modelling process}
% What processes did you undertake to train the models?
%   How did you use the pre-processed data for training?
%   What parameters did you use?
%   Screenshots of code or Minted code blocks necessary.
After the extensive EDA and preprocessing of the data, the refined training set consisting of 80\% of the data 
could then be used to train the five models. No features were removed, as all were deemed to be relevant in
diabetes diagnosis from topical research and EDA. 

% \para While the inner workings of each algorithm differ immensely, Scikit-Learn allows the fitting and prediction of 
% all models to be uniform, using "fit()" methods to fit each model to the training set, and "predict()" methods to run 
% predictions based on the fit. 

\begin{figure}[H]
    \centering 
    \includegraphics[width=\linewidth]{ModelDev/Iteration1/Code/Fitting.png}
    \caption{The code to fit each of the five models.}
    \label{fig:ModelFitting}
\end{figure}


\pagebreak
\section{Results on seen data}
% Discuss evaluating your model on previously seen data i.e. the training set.
% Screenshots of code or Minted code blocks necessary.

To evaluate each model, the metrics previously discussed in Section \ref{sec:ChosenMetrics} 
were logged alongside confusion matrices, which show the amounts of data
each model correctly and incorrectly predicted, divided into true positives, false positives, 
false negatives and true negatives.

\begin{figure}[H]
    \centering 
    \includegraphics[width=\linewidth]{ModelDev/Iteration1/Code/Seen/Matrices.png}
    \caption{The code to predict with, and output the metrics and confusion matrices of each model on the training set.}
    \label{fig:SeenDataCode}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Seen/MetricsIncluded/RF.png}
        \caption{Random Forest}
        \label{fig:RFSeen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Seen/MetricsIncluded/SVC.png}
        \caption{Support Vector Machine}
        \label{fig:SVMSeen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Seen/MetricsIncluded/LR.png}
        \caption{Logistic Regression}
        \label{fig:LRSeen}
    \end{subfigure}
    
    \vspace{1em}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=.9\textwidth]{ModelDev/Iteration1/Matrices/Seen/MetricsIncluded/NB.png}
        \caption{Na\"ive Bayes}
        \label{fig:NBSeen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=.9\textwidth]{ModelDev/Iteration1/Matrices/Seen/MetricsIncluded/KNN.png}
        \caption{KNN}
        \label{fig:KNNSeen}
    \end{subfigure}
    \caption{The confusion matrices and evaluation metrics of each model on seen data.}
    \label{fig:SeenDataMatrices}
\end{figure}

% \para The confusion matrices and metrics show that Random Forest is the best algorithm when applied to the training set,
% achieving the maximum possible result in all metrics. This is followed by KNN, with over 99 in all metrics. After these two,
% however, the remaining models performed poorly in comparison, with SVM having 86.9\% accuracy, 84.8\% recall and an F1-Score of 
% 87.3, and logistic regression achieving 75 or 76 in all metrics. Na\"ive Bayes performed the worst of the models, with 71.9\% accuracy,
% 73.8\% recall, and an F1-Score of 70.8.

% \subsection{Random Forest}
% Random Forest performed perfectly on the training set, achieving 100\% in all metrics, likely due to the fact that it is an ensemble model 
% with the ability to identify correlations in the data, such as those between Insulin and Glucose. If it also performed perfectly on the testing
% data, this would mean the model was overfitted.

% \subsection{Support Vector Machine}
% It is possible the SVM was unable to create the perfect hyperplane in the data to separate the classes. However, 
% the metrics it did achieve (86.9\% accuracy, 85.1\% recall, F1-Score of 87.2) are not bad. An additional reason 
% for this performance is that the SVM was not assigned any parameters, instead left to its defaults.

% \subsection{Logistic Regression}
% Logistic regression performed the second-worst of the models, achieving 76.6\% accuracy, 77.1\% recall and an F1-Score 
% of 76.4. This could be due to the lack of parameters set.

% \subsection{Na\"ive Bayes}
% Na\"ive Bayes performed poorly in relation to other models (74.7\% accuracy, 76.5\% recall, F1-Score of 73.9) because it assumes feature independence,
% which is not true in this dataset due to correlating columns such as BMI and SkinThickness, and Insulin and Glucose.

% \subsection{K-Nearest Neighbours}
% KNN performed exceptionally well (over 99\% in all metrics), as it effectively memorises the data it is trained on when it is fitted,
% meaning that it is not unexpected for it to perform extremely well on the training set.

\begin{longtable}{ | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | }
    \hline
    \cellcolor{blue!25} Model & \cellcolor{blue!25} Accuracy & \cellcolor{blue!25} Recall & \cellcolor{blue!25} F1 Score\\
    \hline
    Random Forest & 100\% & 100\% & 100\\ 
    \hline
    SVM & 86.9\% & 85.1\% & 87.2 \\
    \hline
    LR & 76.6\% & 77.1\% & 76.4\\
    \hline
    NB & 74.7\% & 76.5\% & 73.9\\
    \hline
    KNN & 99.5\% & 99.4\% & 99.5 \\
    \hline
    \caption{The metrics of each algorithm on the seen training data.}\label{tab:Iteration1Training}
\end{longtable}

\subsection{Overall findings}
Random Forest and KNN showed the best performance on the seen data, with perfect or nearly perfect scores across all metrics. 
SVM demonstrated good performance, while Logistic Regression and Naïve Bayes were the least effective. 
Performance on seen data doesn't necessarily translate to good performance on unseen data, and further
evaluation using the unseen testing set was performed to assess the models' true predictive capabilities.


\chapter{Evaluation and further improvements}

\section{Initial results on unseen data}
The models were evaluated against the unseen training set, so that an analysis 
of how they would perform on real data could be gathered.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ModelDev/Iteration1/Code/Unseen/Matrices.png}
    \caption{The code to predict with, and output the metrics and confusion matrices of each model on the testing set.}
    \label{fig:UnseenDataCode}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Unseen/MetricsIncluded/RF.png}
        \caption{Random Forest}
        \label{fig:RFUnseen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Unseen/MetricsIncluded/SVC.png}
        \caption{Support Vector Machine}
        \label{fig:SVMUnseen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Unseen/MetricsIncluded/LR.png}
        \caption{Logistic Regression}
        \label{fig:LRUnseen}
    \end{subfigure}
    
    \vspace{1em}
    
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Unseen/MetricsIncluded/NB.png}
        \caption{Na\"ive Bayes}
        \label{fig:NBUnseen}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{ModelDev/Iteration1/Matrices/Unseen/MetricsIncluded/KNN.png}
        \caption{KNN}
        \label{fig:KNNUnseen}
    \end{subfigure}
    \caption{The confusion matrices and evaluation metrics of each model on unseen data.}
    \label{fig:UnseenDataMatrices}
\end{figure}

\begin{longtable}{ | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | }
    \hline
    \cellcolor{blue!25} Model & \cellcolor{blue!25} Accuracy & \cellcolor{blue!25} Recall & \cellcolor{blue!25} F1 Score\\
    \hline
    Random Forest & 98\% & 97.8\% & 97\\
    \hline
    SVM & 82.6\% & 71.1\% & 76.1\\
    \hline
    LR & 74.3\% & 60.8\% & 63.9\\
    \hline
    NB & 76.8\% & 64.6\% & 67\\
    \hline
    KNN & 95.3\% & 91.7\% & 93.1\\
    \hline
    \caption{The metrics of each algorithm on the unseen testing data.}\label{tab:Iteration1Testing}
\end{longtable}

% \subsection{Random Forest}
% Random Forest performed very well on the testing set, achieving an accuracy of 98\%, recall of 97.8\% and F1-Score of 97.
% This high performance shows it is an excellent predictor.

% \subsection{Support Vector Machine}
% The SVM also performed decently well on the testing set, with an accuracy of 82.6\%, recall of 71.1\%, and F1-Score of 76.1.
% This would still be considered a good model, but pales in comparison to some of the others produced.

% \subsection{Logistic Regression}
% Logistic regression was one of the lower performers, with an accuracy of 74.3\%, recall of 60.8\%, and F1-Score of 63.9.
% These metrics indicate that it is not a very good predictor, especially with its low recall and F1-Scores. Its confusion matrix 
% also shows this, classifying 61 false negatives, which would be extremely dangerous in diabetes diagnosis.

% \subsection{Na\"ive Bayes}
% Na\"ive Bayes performed similarly to logistic regression, but was slightly better performing. It had an accuracy of 76.8\%, recall of 
% 64.6\%, and F1-Score of 67. While this still makes it a poor predictor, it is slightly better than logistic regression.

% \subsection{K-Nearest Neighbours}
% KNN performed very well, with 95.3\% accuracy, 91.7\% recall and F1-Score of 93.1. This makes it a very reliable predictor, only 
% misclassifying 26 rows according to its classification matrix. 

\subsection{Overall findings}
Random Forest and KNN were the best models, with Random Forest slightly edging out KNN.
SVM showed moderate performance, while Logistic Regression and Naïve Bayes were the least effective models by a considerable margin.

\section{Final model results}\label{sec:FinalResults}
An additional iteration of each model was produced using stratified cross-validation and hyperparameter tuning
to improve their performance.
Detailed descriptions of these processes and how they were implemented can be found in Appendix E, with the results of these improved iterations shown 
in Figures \ref{fig:RFIteration2} through \ref{fig:KNNIteration2}.


\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ModelDev/Iteration2/Results/RF.png}
    \caption{The optimal parameters, cross-validated F1 Score, evaluation metrics, and confusion matrix of the Random Forest model.}
    \label{fig:RFIteration2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ModelDev/Iteration2/Results/SVC.png}
    \caption{The optimal parameters, cross-validated F1 Score, evaluation metrics, and confusion matrix of the Support Vector Machine model.}
    \label{fig:SVCIteration2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ModelDev/Iteration2/Results/LR.png}
    \caption{The optimal parameters, cross-validated F1 Score, evaluation metrics, and confusion matrix of the Logistic Regression model.}
    \label{fig:LRIteration2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ModelDev/Iteration2/Results/NB.png}
    \caption{The cross-validated F1 Score, evaluation metrics, and confusion matrix of the Na\"ive Bayes model. There are no optimal 
    parameters for this model as hyperparameter tuning was not performed on it. The reason for this is described in Appendix E.}
    \label{fig:NBIteration2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{ModelDev/Iteration2/Results/KNN.png}
    \caption{The optimal parameters, cross-validated F1 Score, evaluation metrics, and confusion matrix of the K-Nearest Neighbours model.}
    \label{fig:KNNIteration2}
\end{figure}


\chapter{Conclusion}
\section{Summary of results} 
% Discuss the evaluation results for your final models.
% Summarise the entire report:
%   The problem
%   The datasets
%   Your methodology
%   Your results
% Any insights gained?

The introduction of stratified cross-validation and hyperparameter tuning\footnote{Except Na\"ive Bayes, which had no parameters to tune (See Appendix E.)} to the models
resulted in significant improvements across most algorithms. Intriguingly, the Random Forest model was the only one to decrease in all three metrics, with a decrease in 
all three metrics on the testing data, leading to the KNN model slightly outperforming it (F1 score + 0.0026, Accuracy + 0.0017)
due to one more false positive predicted by the Random Forest model. This suggests that the Random Forest's first iteration may have been overfitted. Despite 
this decrease, the model is still an excellent predictor.

\para Every model other than Random Forest and Na\"ive Bayes saw marked improvements in all three evaluation metrics, demonstrating the benefits of cross-validation 
and hyperparameter tuning.

\begin{longtable}{ | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | }
    \hline
    \cellcolor{blue!25} Model & \cellcolor{blue!25} Accuracy & \cellcolor{blue!25} Recall & \cellcolor{blue!25} F1 Score\\
    \hline
    Random Forest & 97.6\% & 96.2\% & 96.5\\
    \hline
    SVM & 85.7\% & 85\% & 80.1 \\
    \hline
    LR & 75\% & 70.5\% & 65.6\\
    \hline
    NB & 73.8\% & 69.5\% & 64.2\\
    \hline
    KNN & 97.8\% & 96.2\% & 96.7 \\
    \hline
    \caption{The metrics of each second-iteration model on the unseen testing data.}\label{tab:Iteration2Results}
\end{longtable}



\begin{longtable}{ | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | p{0.2\textwidth} | }
    \hline
    \cellcolor{blue!25} Model & \cellcolor{blue!25} Accuracy & \cellcolor{blue!25} Recall & \cellcolor{blue!25} F1 Score\\
    \hline
    Random Forest & \cellcolor{red!25} -0.3\% & \cellcolor{red!25}-1.6\% & \cellcolor{red!25} -0.5\\ 
    \hline
    SVM & \cellcolor{green!18}+3.1\% & \cellcolor{green!18}+13.9\% & \cellcolor{green!18}+4\\ 
    \hline
    LR & \cellcolor{green!18}+0.7\% & \cellcolor{green!18}+9.7\% & \cellcolor{green!18}+1.7\\
    \hline
    NB & \cellcolor{red!25}-3\% & \cellcolor{green!18}+4.9\% & \cellcolor{red!25}-2.8\\
    \hline
    KNN & \cellcolor{green!18}+2.5\% & \cellcolor{green!18}+4.5\% & \cellcolor{green!18}+3.6\\
    \hline
    \caption{The differences in each evaluation metric in each second-iteration model on the unseen testing data compared to the first-iteration models.}\label{tab:Improvements}
\end{longtable}

\pagebreak 
\para Overall, this project suceeded in developing and evaluating machine learning models for predicting the presence of diabetes mellitus using clinical data. 
Through extensive exploratory data analysis, data preprocessing, and model development, the following key results were achieved:

\begin{itemize}
    \item Dataset Identification and Integration
    \begin{itemize}
        \item The identification and integration of two datasets (Pima Indian and Frankfurt) resulted in a comprehensive dataset of 2,768 samples. 
    \end{itemize}
    \item Data Preprocessing and EDA
    \begin{itemize}
        \item Assumptions and questions solved by exploratory data analysis revealed critical insights, including the presence of outliers, missing values,
        and class imbalance, which were addressed through constraining outliers to upper and lower quantiles, imputing missing data using KNN, and balancing
        classes with SMOTE.
    \end{itemize}
    \item Model Development and Evaluation
    \begin{itemize}
        % \item KNN was the best-performing model, followed very closely by Random Forest, then SVM, Logistic Regression and Na\"ive Bayes.
        \item KNN was the top-performing model after hyperparameter tuning, achieving 97.8\% accuracy, 96.2\% recall, and an F1-score of 96.6.
        \item Random Forest performed exceptionally well, with 97.7\% accuracy, 96.2\% recall, and an F1-score of 96.5, only slightly behind KNN.
        \item SVM showed significant improvement after tuning, reaching 85.7\% accuracy, 85\% recall, and an F1-score of 80.1.
        \item Logistic Regression demonstrated moderate performance with 75\% accuracy, 70.5\% recall, and an F1-score of 65.6.
        \item Na\"ive Bayes performed the least effectively, with 73.8\% accuracy, 69.5\% recall, and an F1-score of 64.2.
    \end{itemize}
    \item Iterative improvements
    \begin{itemize}
        \item Thorough research into machine learning practices revealed insights of the importance of cross-validation and hyperparameter tuning,
        which greatly improved 3 of the 5 models after implementation.
    \end{itemize}
\end{itemize}

\pagebreak 
\section{Reflection on Individual Learning}
% Nouh has previously stated he will begin here. This section could be 10% of the marks.
% What did you learn by doing this assignment?
% What aspects of the assignment did you enjoy most?
% How will you further hone your skills in future?

% This section alone is about 350 words. Cutting it down could be best.
Over the course of this project, I have thoroughly enjoyed expanding
my Python skills for data science and machine learning with industry standard packages like Pandas, NumPy and Scikit-Learn. 
The project was a massive challenge that seemed insurmountable at times, requiring extensive research into many academic papers and web resources to further my
knowledge of the medical field and machine learning practices. The most challenging part of this project was the use of GridSearchCV on five different
models, though I eventually reached the solution of an iterative for-loop that ran the search on all five in a clean block of code.

\para The topic of this project was one 
that is close to my heart - members of my family suffer from diabetes mellitus and other health complications caused by it,
including heart attacks and strokes. Therefore, I am intensely interested in the applications of my passions in AI and Machine Learning
to make a positive difference, and the knowledge I gained through this project will be a strong asset in doing so.

\para Despite the accomplishments of this project, it is not without flaws - KNN imputation was used on the testing set to address 
the missing data within it. While this did mean that more of the testing set could be used, this potentially comes at the expense of 
the data not being legitimate, as it was synthetically generated, and could have potentially caused overfitting.

\para I have always enjoyed writing code from as young as 11 years old, with Python being my first language.
In University, I made my \href{https://github.com/LewGoesB00M}{personal GitHub account}, which I use for version control and iterative 
saving of my University work, including for this module in the \href{https://github.com/LewGoesB00M/CMP6202}{CMP6202 repository}, 
where every step taken in the production of this report and code is documented across many commits.
% IF ANY FURTHER CHANGES ARE MADE, WORD COUNT ON THE TITLE PAGE MUST BE UPDATED!

\include{AppendixA}
\include{AppendixB}
\include{AppendixC}
\include{AppendixD}
\include{AppendixE}
% Appendices are 3,807 words.

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography


\end{document}