% -------------------------------------------------------------------------------
% Establish page structure & font.
\documentclass[12pt]{report}

\usepackage[total={6.5in, 9in},
	left=1in,
	right=1in,
	top=1in,
	bottom=1in,]{geometry} % Page structure

\usepackage{graphicx} % Required for inserting images
\graphicspath{{images/}} % Any additional images I use (BCU logo, etc) are from here.

\usepackage[utf8]{inputenc} % UTF-8 encoding
\usepackage[T1]{fontenc} % T1 font
\usepackage{float}  % Allows for floats to be positioned using [H], which correctly
                    % positions them relative to their location within my LaTeX code.
\usepackage{subcaption}
\usepackage{amsmath}

% -------------------------------------------------------------------------------
% Declare biblatex with custom Harvard BCU styling for referencing.
\usepackage[
    useprefix=true,
    maxcitenames=2,
    maxbibnames=99,
    style=authoryear,
    dashed=false, 
    natbib=true,
    url=false,
    backend=biber
]{biblatex}

% Additional styling options to ensure Harvard referencing format.
\renewbibmacro*{volume+number+eid}{
    \printfield{volume}
    \setunit*{\addnbspace}
    \printfield{number}
    \setunit{\addcomma\space}
    \printfield{eid}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}

% Declare it as the bibliography source, to be called later via \printbibliography
\addbibresource{Report.bib}

% -------------------------------------------------------------------------------
% To prevent "Chapter N" display for each chapter
\usepackage[compact]{titlesec}
\usepackage{wasysym}
\usepackage{import}

\titlespacing*{\chapter}{0pt}{-2cm}{0.5cm}
\titleformat{\chapter}[display]
{\normalfont\bfseries}{}{0pt}{\Huge}

% -------------------------------------------------------------------------------
% Custom macro to make an un-numbered footnote.

\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

% -------------------------------------------------------------------------------
% Fancy headers; used to show my name, BCU logo and current chapter for the page.
\usepackage{fancyhdr}
\usepackage{calc}
\pagestyle{fancy}

\setlength\headheight{37pt} % Set custom header height to fit the image.

\renewcommand{\chaptermark}[1]{%
    \markboth{#1}{}} % Include chapter name.


% Lewis Higgins - ID 22133848           [BCU LOGO]                [CHAPTER NAME]
\lhead{Lewis Higgins - ID 22133848~~~~~~~~~~~~~~~\includegraphics[width=1.75cm]{BCU}}
\fancyhead[R]{\leftmark}

% ------------------------------------------------------------------------------
% Used to add PDF hyperlinks for figures and the contents page.

\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=black,
}

% ------------------------------------------------------------------------------
\usepackage{xcolor} 
\usepackage{colortbl}
\usepackage{longtable}
\usepackage{amssymb}
% ------------------------------------------------------------------------------
\usepackage{tcolorbox}
\newcommand{\para}{\vspace{8pt}\noindent}
\usepackage{tikz}
% -------------------------------------------------------------------------------

\title{Using supervised learning for the binary classification of Type 2 Diabetes}
\author{Lewis Higgins - Student ID 22133848}
\date{December 2024}

% -------------------------------------------------------------------------------

\begin{document}


\makeatletter
\begin{titlepage}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{BCU}\\[4ex]
        {\huge \bfseries  \@title}\\[50ex]
        {\@author}\\[2ex]
        {CMP6202 - Artificial Intelligence \& Machine Learning}\\[2ex]
        {Module Coordinator: Nouh Elmitwally}\\[10ex]
    \end{center}
\end{titlepage}
\makeatother
\thispagestyle{empty}
\newpage


% Page counter trick so that the contents page doesn't increment it.
\setcounter{page}{0}


\tableofcontents
\thispagestyle{empty}

\pagecolor{yellow}
\begin{abstract}
    Probably would benefit from an abstract. You can't really write this until the very end though,
    so return to it then. \textbf{The example work is from a previous year wherein this assessment was a group task.
    You can see that each group member developed one ML model, but you seem to be developing all of them yourself, so don't be mislead
    by the report titles only mentioning one model.}
\end{abstract}
\pagecolor{white}

\chapter{Introduction}

% You've opened by talking about the UK but then transitioned to the whole world?
% Your datasets are American and German in origin, so it's likely that the worldwide perspective will be better.
Diabetes mellitus, or type 2 diabetes, accounts for 90\% of the 4.4 million cases of diabetes in the UK, and it is estimated that 
there are 1.2 million undiagnosed cases of type 2 diabetes across the country \autocite{diabetes_uk_how_nodate}. The rate of type 
2 diabetes per 100,000 individuals is rapidly increasing, with \textcite{khan_epidemiology_2020}'s analysis projecting that by 
2030, the rate will reach 7,079 per 100,000. Many people with diabetes suffer immensely reduced quality of life, with approximately 50\% 
of patients suffering from peripheral neuropathy \autocite{dhanapalaratnam_effect_2024}, an irreversible disability which causes immense pain due 
to nerve damage from high blood sugar \autocite{nhs_peripheral_2022}, which can occur when the patient was unaware they even 
had diabetes. 

\para
Therefore, it is imperative that systems are put in place to enable the swift diagnosis of diabetes, especially type 2 diabetes 
given its major prevalence. This can be accomplished by training machine learning models on existing clinical datasets 
to identify common trends in those with and without type 2 diabetes. This report will document the planning, development 
and evaluation of multiple machine learning models in their classification of whether individuals have type 2 diabetes based 
on multiple clinical factors, specifically through the stages of:

\begin{itemize}
    \item Dataset Identification
    \item Data Integration
    \item Data Preprocessing
    \item Exploratory Data Analysis (EDA)
    \item Model Development 
    \item Model Evaluation
    \item Research Conclusions
\end{itemize}

\pagebreak 
\section{Dataset Identification}

Machine learning models require large amounts of data to train upon, meaning a dataset must be identified consisting of many 
rows and features. This project identified two datasets which could be integrated into one larger dataset, the first of which being 
the well-reputed Pima Indian Diabetes Database \autocite{uci_machine_learning_pima_nodate}, downloaded from \href{https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database}{Kaggle}, 
a platform for students and researchers alike to download and upload datasets and code for research purposes. The data originates from the National Institute 
of Diabetes and Digestive and Kidney Diseases, who collected this data from Pima Indian\footnote{"Pima Indian" refers to a specific Native American ethnic group rather than people from India.}
women aged 21 and over in hospitals in Phoenix, Arizona, USA, and it has previously seen wide use across academic literature relating to machine learning (\textcite{alzubi_diabetes_2023}, \textcite{zou_construction_2024}, \textcite{joshi_predicting_2021}, \textcite{hayashi_rule_2016}),
where other researchers have also aimed to solve the problem of diabetes classification via supervised learning. This dataset contains 768 rows with 9 features.
% Also cite Kaggle here rather than just hyperlinking it.

\para
This project also includes a second dataset, also from \href{https://www.kaggle.com/datasets/johndasilva/diabetes/data}{Kaggle}, that has been previously used in literature by 
\textcite{zou_construction_2024}. This dataset \autocite{john_dasilva_frankfurt_nodate} is based on data from female patients in Frankfurt, Germany, and includes the same 9 features as the Pima Indian dataset, but includes 2000 rows. By integrating these two datasets into one larger
dataset of 2768 rows, it will be possible to give the machine learning models more data to train upon.

\para Table \ref{tab:Features} details the 9 features seen in both datasets and their descriptions.
% You also need to include their types. Ordinal, nominal, etc as with CMP6230.

\begin{longtable}{ | p{0.3\textwidth} | p{0.5\textwidth} | }
    \hline
    \cellcolor{blue!25}Feature & \cellcolor{blue!25}Description \\
    \hline
    Pregnancies & The number of pregnancies the patient has had. \\
    \hline
    Glucose & Plasma glucose concentration over 2 hours in an oral glucose tolerance test. \\
    \hline
    BloodPressure & Diastolic blood pressure in mm/Hg. \\
    \hline
    SkinThickness & Triceps skin fold thickness (mm) \\
    \hline
    Insulin & 2-hour serum insulin. \\
    \hline
    BMI & Body Mass Index, calculated from the patient's weight and height. \\
    \hline
    DiabetesPedigreeFunction & The product of a function to ascertain the probability of diabetes based on family genetics. \autocite{akmese_diagnosing_2022} \\
    \hline
    Age & The patient's age.\\
    \hline
    Outcome & Whether the patient is likely to develop diabetes.\\
    \hline 
    \caption{The features seen in both datasets.}\label{tab:Features}
\end{longtable}

% An interesting statement was made by Mihai in Week 7.
% He said "You split your data BEFORE EDA to avoid conceptual overfitting".
% This probably should have been considered in the presentation slides.
% The template also is structured in a way where you split before analysis.

\section{Supervised learning task identification}
As previously mentioned, it is possible for patients to have diabetes without knowing. Therefore,
it is paramount that swift and simple diagnosis methods are put in place, which can be achieved 
through the use of supervised learning classification models. This requires the existence of the 
"ground truth", which refers to the label given to data that indicates its class \autocite{c3ai_what_nodate}. 
Within these datasets, the ground truth is present as the 'Outcome' feature, which will be used 
as the target variable for the produced classification models.
% Write more.

% RECALL VERY IMPORTANT DUE TO MEDICAL FIELD!
% SMOTE drawbacks - Synthetic data. Needs data to make more data. In the event of SEVERE imbalance
% (not like your data) it might just give noise instead of useful data.
% KNN + SMOTE - Your dataset is 50% synthetic now. Talk somewhere about the risks of that.
% You removed 98th percentile before imputing your 0s, so was it even the 98th percentile or not?
%   He either didn't notice that or thought it was the right thing to do and didn't mention it so maybe it's fine?
% Split before EDA.
% Why do we do training and testing splits? You were unable to directly answer that during the presentation.

% If you take VSCode screenshots, ensure that your IDE theme is the same across all of them.


\chapter{Exploratory Data Analysis}
This chapter details the EDA processes undertaken with the datasets, including 
key questions that will be answered by the process, as well as the splitting of the 
data into training and testing % and validation
sets.

\section{Data Integration}
% Not included in template but I feel it's necessary to include somewhere because that's what you're doing.
The two datasets must first be merged into one to allow for an overall analysis to be performed.
This is a simple process because they both contain the same 9 features, and is detailed in Figure 
\ref{fig:Integration}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Integration.png}
    \caption{Integrating the two separate datasets into one larger dataset.}
    \label{fig:Integration}
\end{figure}

\pagebreak 

\section{Question identification and assumptions}
% What questions do you have that thorough EDA would answer?
% Example A gives a good example on what this section should look like.
The key factors involved in the diagnosis of diabetes are critical to understand, which 
can be solved through EDA on these datasets. It is possible to make various assumptions based 
on topical background research of each of the features in the dataset, detailed in Table \ref{tab:Assumptions}

\begin{longtable}{ | p{0.19\textwidth} | p{0.75\textwidth} | }
    \hline
    \cellcolor{blue!25} Feature & \cellcolor{blue!25} Research-based assumptions \\
    \hline
    Pregnancies & Approximately 13.4\% of pregnant women develop a temporary condition known as Gestational 
    Diabetes Mellitus (GDM), which typically subsides after birth \autocite{adam_pregnancy_2023}.
    However, research by \autocite{dennison_absolute_2021} indicates that 33\% of women who develop 
    GDM will go on to develop permanent diabetes mellitus within 15 years. Therefore, it is assumed 
    that pregnancies will positively correlate with the diabetes outcome. It is also expected that 
    pregnancies should naturally positively correlate with age. \\
    \hline
    Glucose & Glucose concentrations are an enormous factor in the diagnosis of diabetes mellitus, being one 
    of the main metrics used to certify the condition, where results over 200mg/dL mean an absolute diagnosis\footnote{The other main metric is insulin deficiency, meaning that the patient could have glucose levels lower than 200mg/dL and still be diagnosed if they are instead insulin deficient. \autocite{aftab_cloud-based_2021}.}
    \autocite{aftab_cloud-based_2021}. It is therefore assumed that the glucose concentrations will be one 
    of the strongest influences of the outcome, and that it will also correlate heavily with insulin levels. \\
    \hline
    BloodPressure & Diastolic blood pressure (DBP) does influence the diagnosis of diabetes mellitus, as 
    56.2\% of recently diagnosed patients presented with elevated DBP in \textcite{nelaj_high_2023}'s limited 
    study of 126 patients, but it is not a decisive factor by itself. Therefore, it is assumed that there will 
    be some correlation between DBP and the outcome, but not as major as other factors like plasma glucose levels.\\
    \hline
    SkinThickness & It is a frequent assumption even non-academically that people who weigh more, and by consequence have higher 
    skin thickness in certain areas such as the triceps, have a higher risk of developing conditions like type 2 diabetes. This is 
    backed by a study by \textcite{ruiz-alejos_skinfold_2020}, which found strong associations between skin thickness and diabetes mellitus,
    as well as high blood pressure. Therefore, it is assumed that there will be a strong correlation between tricep skin thickness and the outcome, 
    as well as an expectation of strong correlations between thickness, BMI and blood pressure.\\
    \hline
    Insulin & Diabetes mellitus is directly associated with insulin deficiency, and as such, it is assumed that this factor will be 
    the strongest influence in the outcome. This is because 2-hour serum insulin tests, as used in this dataset, are frequently 
    part of HOMA-IR\footnote{Homeostasis Model Assessment of Insulin Resistance, used to measure insulin resistance \autocite{tahapary_challenges_2022}, which can be used in both type 1 and type 2 diabetes diagnosis \autocite{khalili_are_2023}.}
    assessments. \\
    \hline
    BMI & BMI is likely to be a significant factor in the outcome, which is backed by previous academic studies indicating that 71\% 
    of studied individuals showed increases in BMI prior to diagnosis \autocite{donnelly_trajectories_2024}. Additionally, BMI is used in insulin resistance measurement assessments,
    which are key assessments in diabetes diagnosis, meaning that it is a safe assumption that BMI will be a large factor in the outcome. \\
    \hline
    DiabetesPedigree- Function & People are more likely to develop diabetes mellitus if there is a family genetic history of the condition, though it is not 
    directly caused by any one particular gene \autocite{diabetes_uk_what_nodate}. With the pedigree function aiming 
    to quantify the inheritance probability, it can be assumed that it will likely correlate heavily with the outcome. \\
    \hline
    Age & \textcite{chackrewarthy_age_2012} studied the effects of age as a risk factor for diabetes mellitus, finding that many natural associated 
    factors of ageing including increases in body fat and decreases in lipid metabolism had considerable influence on the development of insulin 
    resistance and diabetes mellitus by consequence. Therefore, it is likely that there will be a noticeable correlation between a patient's age 
    and the outcome. 
     \\
    \hline
    \caption{Research-based assumptions prior to any EDA.}\label{tab:Assumptions}
\end{longtable}

\para Based on these assumptions, the questions that this EDA process aims to answer are:


\begin{longtable}{ | p{0.05\textwidth} | p{0.85\textwidth} | }
    \hline
    \cellcolor{blue!25} ID & \cellcolor{blue!25} Research-based assumptions \\
    \hline
    1 & Are there any missing values or values that are not physically possible?\\
    \hline
    2 & Are there any significant outliers?\\
    \hline 
    3 & Is the dataset evenly balanced in terms of the outcome? If not, what should be done?\\
    \hline 
    4 & Does the rate of diabetes positively correlate with the amount of pregnancies a woman has had?\\
    \hline
    5 & Does the amount of pregnancies influence any of the other features?\\
    \hline
    6 & What is the distribution of blood glucose levels in patients with and without diabetes?\\
    \hline
    7 & Does BMI influence glucose levels?\\
    \hline
    8 & Is diastolic blood pressure a worthwhile diagnosis method in this dataset?\\
    \hline
    9 & Is the average skin thickness of those with diabetes actually higher than those without?\\
    \hline
    10 & How does the relationship between insulin and glucose change between those with and without diabetes?\\
    \hline
    \caption{The questions that this EDA process aims to answer.}\label{tab:Questions}
\end{longtable}

\section{Splitting the dataset}
% How was the data split?
% Why do we split? Why 80:20 as you're doing?
% Train/test? KFold?
% Maybe do train/test at first and KFold in an iterative improvement and compare the models?
% Training and testing sets are a minimum. It's possible they also want a VALIDATION set
It is good practice to first split the data into training and testing sets before performing exploratory data analysis
to avoid conceptual overfitting, also known as data leakage. Conceptual overfitting occurs when insights gained from the entire dataset 
influence model development decisions, which may eventually lead to actual overfitting. By excluding the training data from the analysis,
it effectively simulates a real-world environment where the data being given to the model is not known, even to its developers.

% !!! Give a better and longer definition of overfitting.

\para Splitting the data is a mandatory process when developing supervised learning models, primarily for the prevention of overfitting.
Overfitting is a significant challenge in machine learning, where models can perform exceptionally well on their original training data but 
are unable to generalize to unseen data, making them unsuitable for deployed use. The training set must consist of a large portion of the 
data so that the model has enough information to analyse and discover trends within, whereas the testing set is a smaller, unseen remainder 
of the data that the model's predictions can be evaluated against using various metrics. A key point of determination is the proportions 
of the dataset that should go in each set - there is no 'one-size-fits-all' percentage that can provide the best results for every possible dataset \autocite{sivakumar_trade-off_2024},
and factors such as the size of the dataset play a large part in this. Most commonly, splits are either 70:30 or 80:20 for training and testing sets 
respectively.

\para The integrated dataset for this project is 2,768 rows. This is considered to be a small dataset, and as such, it will be best to 
maximize the size of the training split, so a split of 80\% training and 20\% testing was used, visualized in Figure \ref{fig:TrainTestDiagram}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        full/.style={draw, rectangle, minimum width=10cm, minimum height=1.5cm, fill = lightgray},
        train/.style={draw, rectangle, minimum width=8cm, minimum height=1.5cm, fill = cyan},
        test/.style={draw, rectangle, minimum width=2cm, minimum height=1.5cm, fill = pink},
        arrow/.style={<->, thick}
    ]
    
    % Full dataset block
    \node[full] (full) at (0,1.5) {Full dataset (2768 rows)};
    
    % Training set block
    \node[train] (train) at (-1,0) {Training};
    
    % Testing set block
    \node[test] (test) at (4, 0) {Testing};

    % Proportion indicators
    \draw[arrow] (-5,-1.2) -- (2.9,-1.2) node[midway, below] {80\%};
    \draw[arrow] (3.1,-1.2) -- (4.9,-1.2) node[midway, below] {20\%};
    
    \end{tikzpicture}
    \caption{A visual representation of the train/test split.}
    \label{fig:TrainTestDiagram}
\end{figure}

\para To accomplish this, the data must first be split into $X$ and $y$ tables, where $X$ consists of the eight features, and $y$ is the target variable.
After the data is split to $X$ and $y$, it can be split into training and testing sets through Scikit-Learn's "train\_test\_split" method, as depicted in Figure \ref{fig:TrainTestSplit}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{EDA/TrainTestSplit.png}
    \caption{Splitting the data at an 80:20 ratio.}
    \label{fig:TrainTestSplit}
\end{figure}

\para By default, this method will first shuffle all rows in the dataset before splitting it, which introduces an element of randomness which can damage 
reproducibility. To combat this, the "random\_state" parameter can be set to ensure that the same shuffle will occur every time.

\para When performing EDA, the Outcome column will be necessary to the analysis, so a deep copy\footnote{A complete copy rather than a pointer to the original data}
of X\_train was made with y\_train (the Outcome column) being added to it, merging the two back into a full training set, shown in Figure \ref{fig:FullTrainingSet}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{EDA/FullTrainingSet.png}
    \caption{Duplicating X\_train and adding the Outcome column for EDA.}
    \label{fig:FullTrainingSet}
\end{figure}


\section{EDA process and results}
In the interest of report conciseness, the full EDA process and results accompany this report as Appendix A. They were performed using the Seaborn Python library,
which uses Matplotlib to produce convenient visualisations to answer each question posed in Table \ref{tab:Questions}. The topics and issues identified
during the process are solved in Section \ref{sec:Preprocessing}.


\section{EDA conclusions}
The extensive EDA conducted in Appendix A provided clear answers to each question posed in Table \ref{tab:Questions}, 
which are detailed below in Table \ref{tab:Answers}.

\begin{longtable}{ | p{0.05\textwidth} | p{0.85\textwidth} | }
    \hline
    \cellcolor{blue!25} ID & \cellcolor{blue!25} EDA-based answer \\
    \hline
    1 & There are missing values in the dataset that were not originally presented as such; they were instead
    0s in columns where this would be physically impossible. After converting the impossible 0s to missing values 
    with NumPy, it was established that there were 18 missing Glucose values, 125 missing BloodPressure values,
    800 missing SkinThickness values, 1330 missing Insulin values, and 39 missing BMI values.
    % \begin{itemize}
    %     \item 18 missing Glucose values.
    %     \item 125 missing BloodPressure values.
    %     \item 800 missing SkinThickness values.
    %     \item 1330 missing Insulin values.
    %     \item 39 missing BMI values.
    % \end{itemize}
    \\
    \hline
    2 & The analysis identified significant outliers across various features using box plots. 
    Particularly extremely outliers were noted in the SkinThickness, BMI, and Insulin columns. These outliers
    could affect data scaling and model performance, and will need to be transformed in data cleaning. \\
    \hline 
    3 & The dataset is imbalanced, with 65.5\% of rows in the training set having an outcome of 0, while the remaining 
    35.5\% had an outcome of 1.\\
    \hline 
    4 & The EDA did show a somewhat positive correlation between the number of pregnancies and diabetes outcomes, 
    although pregnancies alone are not decisive in predicting diabetes.\\
    \hline
    5 & Pregnancies are found to correlate with age, as expected, but do not significantly influence other features.\\
    \hline
    6 & There is a clear and significant distinction in glucose levels between diabetic and non-diabetic individuals,
    with higher levels being mostly present in those with diabetes, making it a strong indicator.\\
    \hline
    7 & A slight positive correlation is observed between BMI and glucose levels, indicating that higher BMI could be 
    associated with higher glucose levels.\\
    \hline
    8 & Blood pressure shows some correlation with diabetes outcomes but is not a strong standalone diagnostic factor. 
    However, like pregnancies, previously mentioned academic research specifies that it is still a useful feature to keep.\\
    \hline
    9 & Individuals with diabetes tend to have higher skin thickness on average, although this feature contains significant high outliers
    in both individuals with and without diabetes.\\
    \hline
    10 &  The relationship between insulin and glucose varies between diabetic and non-diabetic individuals, with diabetic individuals 
    showing higher glucose levels as a result of lower insulin levels, which is expected of type 2 diabetes.\\
    \hline
    \caption{Answers to the questions after the completion of the EDA process.}\label{tab:Answers}
\end{longtable}

\pagebreak
\para Overall, the EDA process conducted in Appendix A has been highly beneficial for many reasons. Firstly, it identified critical data issues, 
such as the abundance of missing values and outliers, which are essential to address to improve model accuracy and reliability. By recognizing these 
issues early, the EDA ensures that data preprocessing steps can be effectively planned to mitigate their impact. Secondly, the EDA highlighted significant
correlations between features and the diabetes outcome, such as the strong influence of glucose levels and BMI on diabetes diagnosis. Understanding these
relationships helps in feature selection and engineering, which are crucial for building effective predictive models. Additionally, the EDA revealed class 
imbalances in the dataset, guiding the development of strategies to handle this imbalance during model training to prevent biased predictions. 

\para The insights gained from the EDA therefore provide a solid foundation for informed decision-making in the experimental design and model development, 
ensuring that the models are trained on clean, balanced, and relevant data, ultimately enhancing their predictive performance. This is especially crucial 
given that the models would be used in the medical field, where incorrect predictions could have life-changing ramifications.

\chapter{Experimental Design} % This may benefit from being its own .TeX file.
This chapter details the planned algorithms to be leveraged against this dataset,
as well as the metrics to evaluate them. Furthermore, the data cleaning and preprocessing 
stages will be deeply explored, as well as some potential limitations relating to their use.

\section{Identification of chosen algorithms}\label{sec:ChosenAlgorithms}
% What algorithms? 
% Why those? 
% How do those algorithms work?
Five classification algorithms will be leveraged on this dataset, these being Random Forests,
Support Vector Machines, Logistic Regression, Na\"ive Bayes, and K-Nearest Neighbours.
% Up to this point is 2664 words, because appendices are excluded as confirmed by the brief.
% This could feasibly be an appendix, as you can't possibly keep below ~4400 with this section included.

\subsection{Random Forest}
Random forests are ensemble models, meaning that they leverage multiple other models and average their findings to 
give a single result. The other models used by a random forest are decision trees, which are flowchart-like structures where 
data is split at each node of the tree based on feature values to arrive at a classification. Random forests create many decision 
trees based on randomly sampling the dataset, and these trees then classify rows based on what they learn. Then, the predictions 
for each row by all decision trees are aggregated, and the prediction reached by the most trees is used.
The amount of decision trees used is a parameter that can be set, and the computational requirements of the algorithm 
also increase alongside this amount.
% This was written half-asleep at 12am and likely doesn't make much sense.

\para Random forests are reputed as one of the best classification algorithms, and are used frequently in many fields 
including diabetes diagnosis as seen in works from \textcite{chang_pima_2023} and \textcite{alzubi_diabetes_2023},
demonstrating their suitability for this classification task.
% They're resistant to overfitting, too. Talk about this.

\subsection{Support Vector Machine}
Support vector machines aim to find the optimal hyperplane\footnote{A decision boundary that separates the classes. In a two-dimensional graph, this would be a line of best fit, but in a multidimensional dataset, it would be a hyperplane.}
which separates classes with the highest possible margin, and are commonly used in classification problems \autocite{ibm_what_2023}. Data points on either 
side of the margin are known as support vectors. Figure \ref{fig:OptimalSVM} shows how a perfect SVM would look in a two-dimensional dataset for demonstrative purposes.

\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{Algorithms/OptimalSVM.png}
    \caption{An optimal SVM that demonstrates the algorithm's key features. Rows on the dotted line are the support vectors, the solid line is the hyperplane, and the space
    between the hyperplane and support vectors is the margin. \autocite{kirchner_using_2018}}
    \label{fig:OptimalSVM}
\end{figure}

\para \textcite{kirchner_using_2018} state that SVMs are computationally intensive, requiring lots of memory and processing power 
to run optimally. Additionally, \textcite{atla_sensitivity_2011} mention that SVMs are particularly sensitive to noise in 
datasets, meaning their quality can be poor when there are many outliers. Despite these drawbacks, however, they can still 
be useful in diabetes classification as observed in the work of \textcite{zou_construction_2024}.

\subsection{Logistic Regression}
Logistic regression computes a weighted sum of all input features, where the appropriate weights are learned by the algorithm as it trains.
A sigmoid function, shown in Equation \ref{eq:Sigmoid}, is then used to transform the sum to a value between 0 and 1, which is then used to classify the row as 0 if it is below 
0.5, or classify it as 1 if it is above 0.5. 

\begin{equation}\label{eq:Sigmoid}
    f(x) = \frac{1}{1 + e^{-x}}     
\end{equation}

\para Logistic regression is a very common algorithm in binary classification tasks, as it is simple to implement and interpret, and
can perform well even with limited amounts of training data \autocite{kavya_applications_2024}. It has also been widely used in previous 
analyses of the Pima Indian dataset. (\textcite{alzubi_diabetes_2023}, \textcite{joshi_predicting_2021}, \textcite{zou_construction_2024})

\subsection{Na\"ive Bayes}
Na\"ive Bayes is an example of a probabilistic classification algorithm \autocite{ibm_what_2021}, as it is based on Bayes' theorem,
which calculates the probability of an event given that another event has occurred, shown in Equation \ref{eq:Bayes}. 
In binary classification, it is the probability of the target given the other features. Na\"ive Bayes gets its name from its na\"ive assumption
that features are all independent of each other.

\begin{equation}\label{eq:Bayes}
    P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\end{equation}

\para This theorem calculates that the probability of an event occurring is equal to the probability of the event given 
prior knowledge multiplied by the prior probability of the event. This can be contextualised to this specific dataset, shown in Equation \ref{eq:ContextBayes}.

\begin{equation}\label{eq:ContextBayes}
    P(Diabetes | Other features) = \frac{P(Other features | Diabetes) \cdot P(Diabetes)}{P(Other features)}
\end{equation}

\para Gaussian Na\"ive Bayes is used in this project, as it is best suited to continuous data as seen in this dataset's 
features. Interestingly, Na\"ive Bayes typically has lower accuracy than other models such as Random Forests \autocite{khan_novel_2023},
though it has been used in previous diabetes classification tasks \autocite{aftab_cloud-based_2021, chang_pima_2023, zou_construction_2024}.

\subsection{K-Nearest Neighbours (KNN)}
The k-Nearest Neighbours (kNN) algorithm is a fundamental machine learning classification method that operates by identifying 
the $k$ closest training examples to a data point across all features and assigns the most common class among these neighbours 
as the prediction. $K$ is a parameter that can be set within the algorithm, and has a heavy influence over its accuracy.
KNN doesn't make any assumptions about the distribution of the data, making it useful for complex, real-world scenarios
such as healthcare \autocite{thomas_addressing_2021}, and has also been used on the Pima Indian and Frankfurt datasets
to high success \autocite{alzubi_diabetes_2023, zou_construction_2024}.


\pagebreak
\section{Identification of appropriate evaluation techniques}
% What metrics?
% Why those?
% How do they show the model's performance?
When evaluating classification models, it is crucial to select appropriate metrics that provide a comprehensive understanding of the model's performance.
For this project, three key evaluation metrics will be used: accuracy, recall, and F1 score. 
By using these three metrics in combination, we can gain a comprehensive understanding of our classification models' performance. Accuracy provides an overall measure of correctness,
recall ensures positive cases are not missed, and the F1 score offers a balanced view that accounts for both precision and recall.
This multi-metric approach will allow for a detailed evaluation of the five models.

\subsection{Accuracy}
Accuracy is the most straightforward metric, measuring the overall correctness of the model's predictions. It is calculated as the ratio of correct predictions
(both true positives [TP] and true negatives [TN]) to the total number of predictions, including false positives [FP] and negatives [FN] \autocite{google_classification_nodate},
shown in Equation \ref{eq:Accuracy}.

\begin{equation}\label{eq:Accuracy}
    \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\end{equation}


\para While accuracy is intuitive and easy to interpret, it can be misleading in cases
of imbalanced datasets, which is a concern in this dataset that will need to be addressed in preprocessing.

\subsection{Recall}
Recall, also known as sensitivity or true positive rate, is particularly important in medical diagnosis scenarios like diabetes detection. It measures
the model's ability to correctly identify all positive cases. In our context, high recall ensures that we minimize the number of diabetic patients
who are incorrectly classified as non-diabetic (false negatives). This is crucial because missing a diabetes diagnosis could have serious health implications for the patient such 
as neuropathy, vision problems and other health complications such as heart disease or strokes \autocite{nhs_type_nodate}.

\begin{equation}\label{eq:Recall}
    \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}

\subsection{F1 Score}
The F1 score provides a balanced measure of the model's performance by combining precision\footnote{The rate of correctly classified positives. Often has an inverse relationship with Recall \autocite{google_classification_nodate}.}
and recall into a single metric, and is particularly useful with imbalanced datasets such as the one used in this project.
The F1 score is calculated as the harmonic mean\footnote{The reciprocal of the mean of the reciprocals of the features. A reciprocal is 1 divided by the number, such as 4's reciprocal being 1/4.}
of precision and recall, giving equal weight to both metrics, which is useful because in a medical context, false positives and negatives are extremely important 
and can bear significant consequences.

\begin{equation}\label{eq:Precision}
    \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\end{equation}

\begin{equation}\label{eq:F1Score}
    \text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}



\section{Data Cleaning and Pre-processing Transformations}\label{sec:Preprocessing}
% What data cleaning techniques did you apply and why?
% Discuss:
%   What is data cleaning? (e.g. handling missing data)
%       Imputation via KNN will work because each missing column has a highly correlated associate. Insulin can be inferred from Glucose. SkinThickness from BMI.
%       BloodPressure can't but it's not missing too many.
%   What is data encoding? (label, one-hot)
%   What is data scaling? Why is it done?
%       KNN and SVM - Distance between points. Heavily skewed by large differences.
%   Have any further engineering techniques been applied? Dimensionality reduction?
%       Certainly not dimensionality reduction because you only have 8 features to begin with.
%   How have you handled missing data? KNN Imputation as Zou et al. did because mean and median were catastrophic.

% SCALING - STANDARDSCALER THEN ROBUST IN ITERATIVE IMPROVEMENT?
% SMOTE AND NO SMOTE IN ITERATIVE IMPROVEMENT?

% Significant problem: KNN Imputation of the testing set is not optimal as you're technically then testing the model
% on data it's already been trained on.


\section{Limitations and Options}
% What are the limitations of this approach?
%   The sheer amount of missing insulin data and imputation means the datasets are basically synthetic now.


\chapter{Model Development} % Titled "predictive modelling / model development" in template.
% This may benefit from being its own .TeX file.
This chapter details the training and evaluation processes of the original produced models
before any iterative improvements such as hyperparameter tuning.

\section{Predictive modelling process}
% What processes did you undertake to train the models?
%   How did you use the pre-processed data for training?
%   What parameters did you use?
%   Screenshots of code or Minted code blocks necessary.

\section{Results on seen data}
% Discuss evaluating your model on previously seen data i.e. the training set.
% Screenshots of code or Minted code blocks necessary.

\chapter{Evaluation and further improvements}
This chapter details the extensive evaluation of each model, as well as iterative improvements 
that were made to enhance their performance.
% No idea what this section entails, as the template is quite non-descript on it. (or over-descript idk)
% I believe it's about the iterative improvements to the models?
% You should talk about GridSearchCV and how it allowed for optimal hyperparameter tuning.

\chapter{Conclusion}
% THIS IS NOT A DRAFT. YOU NEED TO PROOFREAD THIS!
% ENSURE IT'S STRUCTURED WELL WITH NO WEIRD PAGE BREAKS.

% You say "will be" a lot, but a report is past tense. Change these to "is".

% Upon reaching this point, go back and make sure your introduction is still valid and makes sense in relation
% to the rest of the document. This was an issue in CMP6230 that you caught last-minute.

% Pay significant attention to the conclusion, especially the individual learning reflection, as Nouh mentioned 
% in previous weeks that he's starting his reading from there and it's a heavy bearing on your grade (10% so 8% overall)

% A few times across the report you mention "the 9 features". I don't think the target variable is a feature, so shouldn't it be
% the 8 features?

\section{Summary of results} 
% You've done this in labs, make a DF of all models and accuracies, plot it on a graph.
% Discuss the evaluation results for your final models.
% Summarise the entire report:
%   The problem
%   The datasets
%   Your methodology
%   Your results
%   Any insights gained?

\section{Reflection on Individual Learning}
% Nouh has previously stated he will begin here. This section could be 10% of the marks.
% What did you learn by doing this assignment?
% What aspects of the assignment did you enjoy most?
% How will you further hone your skills in future?

% You need evidence for this section. Miscellaneous certificates as well as public Github repositories.
% On that topic, many of your repos aren't exactly "sanitised" for public viewing and had many colorful notes in
% that were only meant to be read by you. Pick and choose repos that are fine. Also the GitHub username needs to change,
% there's no way I'm putting "LewGoesB00M" in an official University assignment.

\include{AppendixA}

\printbibliography

\end{document}